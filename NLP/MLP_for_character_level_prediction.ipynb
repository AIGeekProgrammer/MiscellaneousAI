{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OAnvyNv_Qhu2taWfGTiZ2ssVaZvXxedo",
      "authorship_tag": "ABX9TyNrouWSgz7mNYipLkLZbIMH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIGeekProgrammer/MiscellaneousAI/blob/main/NLP/MLP_for_character_level_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook: MLP for character-level predictions<br>\n",
        "Author: Szymon Manduk<br>\n",
        "Date: Nov 7, 2022<br>\n",
        "Description: implementing multilayer perceptron for character-level prediction  - based on the idea presented by A. Karpathy: https://youtu.be/TCH_1BHY58I and the original paper: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf<br>"
      ],
      "metadata": {
        "id": "oHM7GLj0Ewa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6_Zbx7NBTiSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79chBQlgHv2O",
        "outputId": "d929ed67-be24-40f0-da44-4448b7994cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We open the file with names (names.txt), read names into a list while removing newline \n",
        "# and adding character '.' the end of each word.\n",
        "words = []\n",
        "with open('/gdrive/My Drive/Test/names.txt', 'r') as f:\n",
        "  for cnt, line in enumerate(f.readlines()):\n",
        "    words.append(line.rstrip('\\n') + '.')\n",
        "print(words[:5])\n",
        "print(f'Number of words {len(words)}')"
      ],
      "metadata": {
        "id": "W7-kc0QHHv2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4a5422-6d39-453b-e975-27f5f2dc7547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma.', 'olivia.', 'ava.', 'isabella.', 'sophia.']\n",
            "Number of words 32033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We build a list of all unique letters ...\n",
        "s = set()\n",
        "for word in words:\n",
        "  s.update(list(word))\n",
        "letters = sorted(list(s))\n",
        "letters[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ssVFrJGBiOc",
        "outputId": "84bca210-8725-4f69-a627-d336753a3da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... to be able to convert from a letter to a number and vice versa.\n",
        "char2idx = {ch:i for i,ch in enumerate(letters)}\n",
        "idx2char = {i:ch for i,ch in enumerate(letters)}\n",
        "print(f'Ex: Index for f is {char2idx[\"f\"]}')\n",
        "print(f'Ex: Character for index 14 is {idx2char[14]}')\n",
        "print(f'Ex: Character for index 0 is {idx2char[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQGPy19FmcC9",
        "outputId": "afb47e8e-6a08-4c4a-e849-8a3769165dd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ex: Index for f is 6\n",
            "Ex: Character for index 14 is n\n",
            "Ex: Character for index 0 is .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we're ready to build the data and label datasets. We need them in a form of:\n",
        "# - data: list of length, containing subsequent letters from a word\n",
        "# - label: list containing the letter that comes after n letters in data.\n",
        "# Let's start with the first few words only to check if it works properly.\n",
        "\n",
        "x, y = [], []  # data and labels lists\n",
        "\n",
        "for _ in range(5):  # let's randomly check 5 words\n",
        "  length = torch.randint(3, 7, (1,)).item()  # just for testing: randomly choose how many characters do we want to use predicting the next character\n",
        "  word = words[torch.randint(0, len(words), (1,)).item()]  # randomly choose word index\n",
        "  \n",
        "  data = '.' * length  # fill data with dots\n",
        "  for ch in word:\n",
        "    y.append(ch) # append label\n",
        "    x.append(data)  # append data\n",
        "    data = data[1:] + ch  # alter data so that it contains one (more) character from the word\n",
        "      \n",
        "  # Debugging results\n",
        "  print(f'Word: {word}')\n",
        "  print(f'Length of n-gram: {length}')\n",
        "  print(f'Data: {x}')\n",
        "  print(f'Labels: {y}\\n')\n",
        "  x, y = [], []"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlkRs4Ijwepf",
        "outputId": "1cd8421a-c530-43e1-a6fc-b505daa119bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: anne.\n",
            "Length of n-gram: 5\n",
            "Data: ['.....', '....a', '...an', '..ann', '.anne']\n",
            "Labels: ['a', 'n', 'n', 'e', '.']\n",
            "\n",
            "Word: hendrik.\n",
            "Length of n-gram: 6\n",
            "Data: ['......', '.....h', '....he', '...hen', '..hend', '.hendr', 'hendri', 'endrik']\n",
            "Labels: ['h', 'e', 'n', 'd', 'r', 'i', 'k', '.']\n",
            "\n",
            "Word: vinny.\n",
            "Length of n-gram: 5\n",
            "Data: ['.....', '....v', '...vi', '..vin', '.vinn', 'vinny']\n",
            "Labels: ['v', 'i', 'n', 'n', 'y', '.']\n",
            "\n",
            "Word: mikah.\n",
            "Length of n-gram: 5\n",
            "Data: ['.....', '....m', '...mi', '..mik', '.mika', 'mikah']\n",
            "Labels: ['m', 'i', 'k', 'a', 'h', '.']\n",
            "\n",
            "Word: spencer.\n",
            "Length of n-gram: 4\n",
            "Data: ['....', '...s', '..sp', '.spe', 'spen', 'penc', 'ence', 'ncer']\n",
            "Labels: ['s', 'p', 'e', 'n', 'c', 'e', 'r', '.']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can create data and label lists for the whole dataset, assuming the length is 3 characters.\n",
        "\n",
        "# Let's measure time elapsed - just for practise.\n",
        "from timeit import default_timer as timer\n",
        "start = timer()\n",
        "\n",
        "x, y = [], []  # data and labels lists\n",
        "length = 3\n",
        "\n",
        "for word in words:  \n",
        "  data = '.' * length  # fill data with dots\n",
        "  for ch in word:\n",
        "    y.append(ch) # append label\n",
        "    x.append(data)  # append data\n",
        "    data = data[1:] + ch  # alter data so that it contains one (more) character from the word\n",
        "\n",
        "end = timer()\n",
        "print(f'Dataset contains {len(y)} elements, computed within {end-start:.2f} seconds.')"
      ],
      "metadata": {
        "id": "iqFJTNBWDpeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae615705-e1be-4d66-f553-80cf540fcdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 228146 elements, computed within 0.24 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at an example batch.\n",
        "print(x[10220:10227])\n",
        "print(y[10220:10227])"
      ],
      "metadata": {
        "id": "PcP-1xxzDpRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "088ae6df-0528-4fa3-d7dd-e72a873d7f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lyn', '...', '..a', '.ad', 'adi', 'dil', 'ily']\n",
            "['.', 'a', 'd', 'i', 'l', 'y', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We convert characters in data and labels into numbers and from lists to tensors\n",
        "data = torch.empty((len(y), length), dtype=torch.int64)\n",
        "labels = torch.empty((len(y), ), dtype=torch.int64)\n",
        "print(data.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "for i, trigram in enumerate(x):\n",
        "  for j, ch in enumerate(trigram):\n",
        "    data[i,j] = char2idx[ch]\n",
        "for i, label in enumerate(y):\n",
        "  labels[i] = char2idx[label]\n",
        "# print(data[10220:10227])\n",
        "# print(labels[10220:10227])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZGv8-4w4Cem",
        "outputId": "1d74ef23-6f31-487b-9d25-0d2f31b806c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([228146, 3])\n",
            "torch.Size([228146])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wYIUt8RC_Vp",
        "outputId": "64f8fd0c-195b-4f21-b9d5-0f1ea652055a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  0,  0],\n",
              "        [ 0,  0,  5],\n",
              "        [ 0,  5, 13],\n",
              "        ...,\n",
              "        [26, 26, 25],\n",
              "        [26, 25, 26],\n",
              "        [25, 26, 24]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXrDrsLgDDsa",
        "outputId": "ebecd3b0-47bd-41cd-b97f-4bef32f5c042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5, 13, 13,  ..., 26, 24,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# As the idea of this notebook is based on this work: Bengio et al. 2003 MLP language model paper (pdf): https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf,\n",
        "# we need to create an embedding table in which we will embedd each character from our data tensor into N-dimentional embedding.\n",
        "# Here we use 2-dimentional embeddings for each of our letters.\n",
        "emb_dim = 2\n",
        "emb = torch.randn(len(letters), emb_dim, requires_grad=True)\n",
        "for i, e in enumerate(emb):\n",
        "  print(f'{i} : {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9oJXCv4CbV",
        "outputId": "d338e5d3-2dbe-4fd6-d1f9-a1ea399d0989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : tensor([0.2693, 1.7548], grad_fn=<UnbindBackward0>)\n",
            "1 : tensor([-1.2506, -1.6556], grad_fn=<UnbindBackward0>)\n",
            "2 : tensor([1.2044, 0.9200], grad_fn=<UnbindBackward0>)\n",
            "3 : tensor([-0.4333, -0.2287], grad_fn=<UnbindBackward0>)\n",
            "4 : tensor([-0.0069, -0.4519], grad_fn=<UnbindBackward0>)\n",
            "5 : tensor([-0.8248, -0.2599], grad_fn=<UnbindBackward0>)\n",
            "6 : tensor([-0.3566, -0.5098], grad_fn=<UnbindBackward0>)\n",
            "7 : tensor([ 1.1725, -0.1151], grad_fn=<UnbindBackward0>)\n",
            "8 : tensor([-0.7304, -1.2691], grad_fn=<UnbindBackward0>)\n",
            "9 : tensor([-0.3946, -0.1323], grad_fn=<UnbindBackward0>)\n",
            "10 : tensor([-0.7607, -1.3150], grad_fn=<UnbindBackward0>)\n",
            "11 : tensor([-1.0425, -0.3478], grad_fn=<UnbindBackward0>)\n",
            "12 : tensor([-1.3718, -0.7268], grad_fn=<UnbindBackward0>)\n",
            "13 : tensor([0.1731, 0.5872], grad_fn=<UnbindBackward0>)\n",
            "14 : tensor([0.8870, 0.1290], grad_fn=<UnbindBackward0>)\n",
            "15 : tensor([0.0258, 0.1462], grad_fn=<UnbindBackward0>)\n",
            "16 : tensor([-0.6157,  0.8806], grad_fn=<UnbindBackward0>)\n",
            "17 : tensor([-0.0925, -0.6717], grad_fn=<UnbindBackward0>)\n",
            "18 : tensor([1.1141, 1.3570], grad_fn=<UnbindBackward0>)\n",
            "19 : tensor([-1.0693,  1.5412], grad_fn=<UnbindBackward0>)\n",
            "20 : tensor([2.5847, 0.2638], grad_fn=<UnbindBackward0>)\n",
            "21 : tensor([-0.1047,  1.3761], grad_fn=<UnbindBackward0>)\n",
            "22 : tensor([ 0.3801, -0.4145], grad_fn=<UnbindBackward0>)\n",
            "23 : tensor([0.2669, 0.5328], grad_fn=<UnbindBackward0>)\n",
            "24 : tensor([ 0.0978, -0.3024], grad_fn=<UnbindBackward0>)\n",
            "25 : tensor([1.1030, 0.8068], grad_fn=<UnbindBackward0>)\n",
            "26 : tensor([0.1596, 1.5265], grad_fn=<UnbindBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can pick any arbitrary data in our training dataset (here: datapoints at positions from 10220 to 10227,\n",
        "# which constitues a 3-grams: 'lyn', '...', '..a', '.ad', 'adi', 'dil', 'ily') ...\n",
        "example = [10220, 10221, 10222, 10223, 10224, 10225, 10226, 10227]\n",
        "print(f'Data:\\n {data[example]}\\n')\n",
        "# ... and we can get the corresponding embeddings.\n",
        "print(f'Correspoding embeddings:\\n {emb[data[example]]} \\n with the shape of: {emb[data[example]].shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgS--cCeWm3o",
        "outputId": "508aa15d-743f-451d-ab98-90bd7d47d82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:\n",
            " tensor([[12, 25, 14],\n",
            "        [ 0,  0,  0],\n",
            "        [ 0,  0,  1],\n",
            "        [ 0,  1,  4],\n",
            "        [ 1,  4,  9],\n",
            "        [ 4,  9, 12],\n",
            "        [ 9, 12, 25],\n",
            "        [12, 25, 14]])\n",
            "\n",
            "Correspoding embeddings:\n",
            " tensor([[[-1.3718, -0.7268],\n",
            "         [ 1.1030,  0.8068],\n",
            "         [ 0.8870,  0.1290]],\n",
            "\n",
            "        [[ 0.2693,  1.7548],\n",
            "         [ 0.2693,  1.7548],\n",
            "         [ 0.2693,  1.7548]],\n",
            "\n",
            "        [[ 0.2693,  1.7548],\n",
            "         [ 0.2693,  1.7548],\n",
            "         [-1.2506, -1.6556]],\n",
            "\n",
            "        [[ 0.2693,  1.7548],\n",
            "         [-1.2506, -1.6556],\n",
            "         [-0.0069, -0.4519]],\n",
            "\n",
            "        [[-1.2506, -1.6556],\n",
            "         [-0.0069, -0.4519],\n",
            "         [-0.3946, -0.1323]],\n",
            "\n",
            "        [[-0.0069, -0.4519],\n",
            "         [-0.3946, -0.1323],\n",
            "         [-1.3718, -0.7268]],\n",
            "\n",
            "        [[-0.3946, -0.1323],\n",
            "         [-1.3718, -0.7268],\n",
            "         [ 1.1030,  0.8068]],\n",
            "\n",
            "        [[-1.3718, -0.7268],\n",
            "         [ 1.1030,  0.8068],\n",
            "         [ 0.8870,  0.1290]]], grad_fn=<IndexBackward0>) \n",
            " with the shape of: torch.Size([8, 3, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we move into constructing a neural network that takes 3 characters (converted into ints and then converted into embeddings like above).\n",
        "# Those embeddings are then transformed into a hidden layer of size h_size, then activated by tanh activation function.\n",
        "# Then a signal goes to the output layer with 27 neurons (corresponding to 27 letters).\n",
        "# The output is softmaxed to get probability distribution of the next character. \n",
        "# A loss can be calculated with CrossEntropy function and the whole model \n",
        "# might be backpropagated up to embeddings, so that they will be fitted to the training dataset.\n",
        "\n",
        "h_size = 100  # the size of the hidden layer\n",
        "\n",
        "# 3 characters embeded in the 2-dim space of embeddings gives us 6 inputs mapped on to h_size neurons in the hidden layer\n",
        "W1 = torch.randn((length*emb_dim, h_size), requires_grad=True)  \n",
        "b1 = torch.randn(h_size, requires_grad=True)  # bias\n",
        "print(f'W1 shape: {W1.shape}, type: {W1.dtype}. Bias shape: {b1.shape}')"
      ],
      "metadata": {
        "id": "bW1J6zGsdZwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a038ba16-61d6-4e77-99d3-2bd658d3bc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 shape: torch.Size([6, 100]), type: torch.float32. Bias shape: torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want be able to feed our model with vectors of size embeding dimention \n",
        "# (here emb_dim=2) times n-gram length (here length=3) that equals 6 elements.\n",
        "# Let's take a look at the shape.\n",
        "emb[data[[example[3]]]].view(-1, length*emb_dim).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4j4ekBWjI2V",
        "outputId": "8ac1c77c-83ff-41da-8de6-86526373d7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And at some data as well.\n",
        "emb[data[[example[3]]]].view(-1, length*emb_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_fPoXARl5vv",
        "outputId": "e15b900b-2cd3-42f3-8a11-f09a9f316979"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2693,  1.7548, -1.2506, -1.6556, -0.0069, -0.4519]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Every 3-gram embedded into 2 dimentions - so the size is 1x6 - will be in our model \n",
        "# multiplied by weigths matix of 6x100 and a bias will be added to each element.\n",
        "emb[data[[example[3]]]].view(-1, length*emb_dim) @ W1 + b1"
      ],
      "metadata": {
        "id": "_VB0sJeDdZqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86886bd7-9efc-4380-da12-8f468c7d01e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.4881,  3.8473, -0.0119,  9.2306,  3.9041, -0.1487,  1.6263, -3.8299,\n",
              "          3.6102,  0.1941,  3.5407,  2.7260,  2.1970,  1.3020,  0.6092,  0.7827,\n",
              "         -0.5906,  0.6666,  0.4678, -2.3617,  4.8591, -0.0197, -7.4429, -2.6018,\n",
              "          0.7172,  3.2309, -0.7379,  2.4612,  1.7530,  2.0906,  1.3643, -3.6151,\n",
              "         -0.7175, -0.1929, -1.1716,  0.4135, -0.7684,  2.3626,  2.8788,  3.5981,\n",
              "          2.1995, -2.5262,  0.3396, -2.4847, -3.5211, -2.4184, -1.9381,  7.0572,\n",
              "          2.5646, -2.8520, -1.7390,  1.3424,  2.5286,  0.3043,  0.5418,  0.0266,\n",
              "         -0.2760, -1.8059, -1.3788,  3.5581, -1.9506,  1.3210, -1.1592, -1.9944,\n",
              "         -0.9448, -2.4289,  3.1992,  3.2059, -5.5499,  0.5655,  1.2340,  1.0847,\n",
              "          0.3649,  4.8948,  1.9556, -5.8657,  2.1201, -0.9133,  0.5383, -0.9931,\n",
              "         -0.9490, -0.3868, -4.9691,  0.8360,  0.0500,  6.6749, -0.2155,  0.1442,\n",
              "          2.2331,  0.8402,  2.3271, -1.8695,  0.1813, -4.0767, -0.7146, -7.2722,\n",
              "         -4.0564,  2.8022,  3.1099, -1.2499]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The output layer will be transforming a signal from hideen size of h_size=100 \n",
        "# into size of 27 possible letters len(letters).\n",
        "W2 = torch.randn((h_size, len(letters)), requires_grad=True)\n",
        "b2 = torch.randn(len(letters), requires_grad=True)\n",
        "print(W2.shape)\n",
        "print(b2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI1vMEK1V7cC",
        "outputId": "1ec6a581-fe67-4d54-f086-f74cb2264818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 27])\n",
            "torch.Size([27])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we do a forward pass and then backpropagate, we need to collect all \n",
        "# learnable parameters, so that we can zero-grad and update them later on.\n",
        "parameters = [emb, W1, b1, W2, b2]  \n",
        "# Counting the number of model's parameters.\n",
        "print(f'The model has altogether {sum(param.nelement() for param in parameters)} learnable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVclZDoZVO9q",
        "outputId": "e2636e2d-0fb0-472c-cdb9-204faa3806d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has altogether 3481 learnable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OK, let's try to do a single forward-backwards pass.\n",
        "\n",
        "# First, we pick a random number for index of data that we are going to use in this pass.\n",
        "idx = torch.randint(0, len(data), (1,)).item()\n",
        "\n",
        "# We get data and label for the randomly chosen index (x and y).\n",
        "x = emb[data[idx]].view(-1, length*emb_dim)\n",
        "y = labels[idx].unsqueeze(0)  # to get 1-dim tensor that will be expected by cross_entropy instead of 0-dmin\n",
        "print(f'Index: {idx}. Label: {y.item()}. Data shape: {x.shape}. Label shape: {y.shape}')\n",
        "\n",
        "# We do a forward pass.\n",
        "hidden = torch.tanh(x @ W1 + b1)\n",
        "logits = hidden @ W2 + b2\n",
        "pred = torch.softmax(logits, dim=1)\n",
        "print(f'Logits:\\n{logits}')\n",
        "print(f'Prediction: {torch.argmax(pred).item()}. Should be {y.item()}')\n",
        "\n",
        "# The we use the CrossEntropy function to calculate a loss.\n",
        "from torch.nn.functional import cross_entropy\n",
        "loss = cross_entropy(logits, y)\n",
        "print(f'Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MkknMsrUsTs",
        "outputId": "19867637-bb86-4114-d1e8-8a178d4e16f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index: 184696. Label: 9. Data shape: torch.Size([1, 6]). Label shape: torch.Size([1])\n",
            "Logits:\n",
            "tensor([[  6.8977,   4.5065,   9.8441,  -5.2872,  12.1559,   3.6251,  -0.5758,\n",
            "          -0.8921,  11.1050,  -3.4304,  -8.1241, -12.6601,   8.7615,   0.9100,\n",
            "          -2.2228,  -8.3410, -20.7316, -11.0152,  12.0085, -19.4439,   2.2679,\n",
            "           0.9950,  11.8976,  -9.6503, -10.7528,  -7.7249, -12.6596]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "Prediction: 4. Should be 9\n",
            "Loss: 16.72532081604004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And now it's time for the backward pass.\n",
        "\n",
        "# First, we zero gradients.\n",
        "for param in parameters:\n",
        "  param.grad = None\n",
        "\n",
        "# Then we go backwards.\n",
        "loss.backward()  "
      ],
      "metadata": {
        "id": "u2Xw1WY7Av1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the gradients for our b1 biases.\n",
        "for param in parameters[2:3]: \n",
        "  print(param.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRSPxn0fXR1L",
        "outputId": "69bf455c-f28a-4e89-efca-1560c40b1742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2.7315e-04,  3.2323e-01, -1.0227e-01, -1.5861e-03, -4.3172e-04,\n",
            "        -2.7308e-04, -8.2726e-03, -6.7851e-01,  4.1354e-02,  3.6165e-06,\n",
            "        -5.1492e-04,  1.4869e-04, -1.4341e-02,  1.0049e-01, -2.0725e-05,\n",
            "         5.4841e-01, -3.4981e-03,  1.5697e-01, -1.8576e-01,  4.2770e-02,\n",
            "        -2.0159e-04, -9.8106e-03, -1.1091e-01, -6.2708e-01, -4.5198e-02,\n",
            "        -1.1895e+00,  2.1187e-01,  5.8905e-01,  1.5224e-03,  4.8668e-02,\n",
            "        -2.4546e-03, -1.5316e-03,  2.9047e-04,  3.1712e-04,  4.4874e-02,\n",
            "        -9.3830e-03,  2.3832e-01, -2.5587e-02,  4.5306e-03,  4.5509e-02,\n",
            "         2.4533e-04, -4.2589e-02, -4.3642e-01, -4.0877e-01, -6.8858e-03,\n",
            "        -5.3409e-03,  3.2786e-02, -1.7206e-02,  1.6420e-04,  8.4218e-03,\n",
            "        -4.9038e-01, -2.1529e-03, -6.2331e-02, -1.3257e-02,  3.1162e-02,\n",
            "        -2.7716e-01, -1.2623e-02, -2.0678e+00,  2.3530e-03,  7.6653e-04,\n",
            "         1.8086e-01,  5.4529e-01, -2.1853e-01, -2.0314e+00,  5.3021e-03,\n",
            "        -9.3283e-03,  7.1348e-04,  3.1051e-05, -8.8898e-02,  8.4733e-01,\n",
            "        -9.1860e-02,  2.0144e+00,  2.3648e-01,  7.3584e-03, -6.3367e-01,\n",
            "        -2.0495e-03,  8.8015e-05,  9.8201e-04, -5.4698e-02,  3.8462e-02,\n",
            "        -3.3373e-04, -1.5262e-01,  6.9666e-02, -5.0024e-02, -1.7915e-01,\n",
            "        -1.1651e-03, -4.2530e-02,  4.1019e-03,  2.2142e-02, -6.8937e-02,\n",
            "        -4.6152e-02, -1.3979e-02,  5.6914e-01,  2.4701e-07,  5.9355e-04,\n",
            "         2.1110e-05, -1.6723e-03, -1.1896e-02,  1.0527e-01, -5.3559e-05])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we do parameters update using some learning rate.\n",
        "learning_rate = 0.3\n",
        "for param in parameters:\n",
        "  param.data += -learning_rate * param.grad"
      ],
      "metadata": {
        "id": "Jgb3J_0O95RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And that's it - we did it!\n",
        "# What we may do right now, as a next step, is to enclose the above passes in a loop.\n",
        "# And also to train on a batch of data instead of a single datapoint.\n",
        "batch_size = 1000\n",
        "epochs = 50000\n",
        "learning_rate = 1"
      ],
      "metadata": {
        "id": "vgqpkyqvdZh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, epochs+1):\n",
        "  # Let's randomly choose a batch of random indexes from the training dataset. \n",
        "  # We need batch_size of indexes.\n",
        "  idx = torch.randint(0, len(data), (batch_size,))\n",
        "  \n",
        "  # We get the data and labels for the randomly chosen batch of indexes.\n",
        "  x = emb[data[idx]].view(-1, length*emb_dim)\n",
        "  y = labels[idx]\n",
        "\n",
        "  # We do a forward pass.\n",
        "  hidden = torch.tanh(x @ W1 + b1)\n",
        "  logits = hidden @ W2 + b2\n",
        "  loss = cross_entropy(logits, y)\n",
        " \n",
        "  # Some debugging.\n",
        "  if epoch % (epochs // 10) == 0:\n",
        "    print(f'Loss at epoch #{epoch}: {loss.item()}')\n",
        "\n",
        "  # We do a backward pass.\n",
        "  for param in parameters:\n",
        "    param.grad = None\n",
        "  loss.backward()\n",
        "  \n",
        "  # We do parameters update.\n",
        "  for param in parameters:\n",
        "    param.data += -learning_rate * param.grad\n",
        "\n",
        "  # To impove training, we add some learning rate decay.\n",
        "  # After 50% of epochs we divide learning rate by 10. \n",
        "  if epoch == epochs // 2:\n",
        "    learning_rate *= 0.1\n",
        "    print(f'\\n Changing learing rate to: {learning_rate}\\n')\n",
        "  # And we cut it further by half after 90% of epochs passed.\n",
        "  if epoch == (epochs // 10) * 9:\n",
        "    learning_rate *= 0.5\n",
        "    print(f'\\n Changing learing rate to: {learning_rate}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a306f5e-faea-40bf-94a3-84918429fe86",
        "id": "JcqIFvGXaQ_Z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at epoch #5000: 2.3173444271087646\n",
            "Loss at epoch #10000: 2.368964433670044\n",
            "Loss at epoch #15000: 2.3065497875213623\n",
            "Loss at epoch #20000: 2.2650322914123535\n",
            "Loss at epoch #25000: 2.243694305419922\n",
            "\n",
            " Changing learing rate to: 0.1\n",
            "\n",
            "Loss at epoch #30000: 2.2050209045410156\n",
            "Loss at epoch #35000: 2.19743013381958\n",
            "Loss at epoch #40000: 2.1508591175079346\n",
            "Loss at epoch #45000: 2.248635768890381\n",
            "\n",
            " Changing learing rate to: 0.05\n",
            "\n",
            "Loss at epoch #50000: 2.2356796264648438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We should be able to get a desent loss at around 2.27 (with around 50 000 epochs)\n",
        "# Let's try to impove our model by:\n",
        "# a) increasing the size of embeddings from 2 to 4,\n",
        "# b) and creating a deeper net that will add some more flexibility to our model.\n",
        "# We will also split data into train, validation and test datasets and all of that in some more Pytorch way of doing the stuff."
      ],
      "metadata": {
        "id": "nM02tOC_UKUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We still have our data and labels in variables 'data' and 'labels'. Let's double check this\n",
        "print(f'Data size: {data.shape}')\n",
        "print(f'Labels size: {labels.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwFhcTZi74Qu",
        "outputId": "28f5e1ca-39ad-4552-d3c0-f66ea96c4696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size: torch.Size([228146, 3])\n",
            "Labels size: torch.Size([228146])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pytorch has a nice class TensorDataset to convert pair of data-labels \n",
        "# into the Dataset object that we may later on use with DataLoader\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
        "\n",
        "dataset = TensorDataset(data, labels)\n",
        "print(f'Type of the <dataset> variable is {type(dataset)}')\n",
        "print(f'The length of the dataset is: {len(dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKOGm4n59yAs",
        "outputId": "40c70b08-9ff1-4e34-bb1b-bcc8cec6c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of the <dataset> variable is <class 'torch.utils.data.dataset.TensorDataset'>\n",
            "The length of the dataset is: 228146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's split data into training (80%), validation (10%) and test(10%) datasets.\n",
        "# Pytorch has useful helper function to achieve this - random_split.\n",
        "# Unfortunately, at the time of writing this notebook, Google Colab uses torch 1.12\n",
        "# and in 1.12 random_split does not accept splits as percentage of original dataset.\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zgCc9TR4VaQA",
        "outputId": "d64dbe01-fb08-48fb-8c05-e6bc3c02683d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We may however copy-paste the code from 1.13 from here: https://pytorch.org/docs/1.12/_modules/torch/utils/data/dataset.html#random_split.\n",
        "import math\n",
        "from torch import default_generator, randperm\n",
        "from torch._utils import _accumulate\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "def random_split(dataset, lengths,\n",
        "                 generator=default_generator):\n",
        "    r\"\"\"\n",
        "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
        "\n",
        "    If a list of fractions that sum up to 1 is given,\n",
        "    the lengths will be computed automatically as\n",
        "    floor(frac * len(dataset)) for each fraction provided.\n",
        "\n",
        "    After computing the lengths, if there are any remainders, 1 count will be\n",
        "    distributed in round-robin fashion to the lengths\n",
        "    until there are no remainders left.\n",
        "\n",
        "    Optionally fix the generator for reproducible results, e.g.:\n",
        "\n",
        "    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
        "    >>> random_split(range(30), [0.3, 0.3, 0.4], generator=torch.Generator(\n",
        "    ...   ).manual_seed(42))\n",
        "\n",
        "    Args:\n",
        "        dataset (Dataset): Dataset to be split\n",
        "        lengths (sequence): lengths or fractions of splits to be produced\n",
        "        generator (Generator): Generator used for the random permutation.\n",
        "    \"\"\"\n",
        "    if math.isclose(sum(lengths), 1) and sum(lengths) <= 1:\n",
        "        subset_lengths: List[int] = []\n",
        "        for i, frac in enumerate(lengths):\n",
        "            if frac < 0 or frac > 1:\n",
        "                raise ValueError(f\"Fraction at index {i} is not between 0 and 1\")\n",
        "            n_items_in_split = int(\n",
        "                math.floor(len(dataset) * frac)  # type: ignore[arg-type]\n",
        "            )\n",
        "            subset_lengths.append(n_items_in_split)\n",
        "        remainder = len(dataset) - sum(subset_lengths)  # type: ignore[arg-type]\n",
        "        # add 1 to all the lengths in round-robin fashion until the remainder is 0\n",
        "        for i in range(remainder):\n",
        "            idx_to_add_at = i % len(subset_lengths)\n",
        "            subset_lengths[idx_to_add_at] += 1\n",
        "        lengths = subset_lengths\n",
        "        for i, length in enumerate(lengths):\n",
        "            if length == 0:\n",
        "                warnings.warn(f\"Length of split at index {i} is 0. \"\n",
        "                              f\"This might result in an empty dataset.\")\n",
        "\n",
        "    # Cannot verify that dataset is Sized\n",
        "    if sum(lengths) != len(dataset):    # type: ignore[arg-type]\n",
        "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
        "\n",
        "    indices = randperm(sum(lengths), generator=generator).tolist()  # type: ignore[call-overload]\n",
        "    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]"
      ],
      "metadata": {
        "id": "PHu014g2VLv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [0.8, 0.1, 0.1])\n",
        "print(f'The lengths of train, validation and test datasets are respectively: {len(train_dataset)}, {len(val_dataset)} and {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaShHd3Q9x6t",
        "outputId": "867536e6-ed76-4058-d5f9-8b151249159e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lengths of train, validation and test datasets are respectively: 182517, 22815 and 22814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's redefine batch size\n",
        "batch_size = 1024"
      ],
      "metadata": {
        "id": "7KfftvKvaXyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Having datasets ready we may wrap them using DataLoader class.\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "test_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "Z-ib_zDG9xzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's try our data loader\n",
        "for i, (x, y) in enumerate(train_dl):\n",
        "  print(x.shape, y.shape)\n",
        "  print(x)\n",
        "  print(y)\n",
        "  if i == 2:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAltjLG39xrE",
        "outputId": "816e1995-445f-49e9-a713-106289695e10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 3]) torch.Size([1024])\n",
            "tensor([[11,  5, 14],\n",
            "        [ 0, 10,  9],\n",
            "        [ 0,  0, 11],\n",
            "        ...,\n",
            "        [ 5, 12,  5],\n",
            "        [25, 26,  1],\n",
            "        [12,  9, 22]])\n",
            "tensor([26,  1,  1,  ...,  1,  0,  5])\n",
            "torch.Size([1024, 3]) torch.Size([1024])\n",
            "tensor([[ 0,  0,  5],\n",
            "        [ 1, 25, 12],\n",
            "        [ 0,  1, 14],\n",
            "        ...,\n",
            "        [ 0,  0, 13],\n",
            "        [ 0, 21, 12],\n",
            "        [ 0, 25,  1]])\n",
            "tensor([13,  5, 18,  ..., 25, 25, 14])\n",
            "torch.Size([1024, 3]) torch.Size([1024])\n",
            "tensor([[ 0, 22,  1],\n",
            "        [ 9, 12, 25],\n",
            "        [ 0,  0,  0],\n",
            "        ...,\n",
            "        [15,  6,  9],\n",
            "        [10, 23,  1],\n",
            "        [ 0,  0,  0]])\n",
            "tensor([12, 14,  5,  ...,  1,  0, 12])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# At this point we're ready to build a new model with increased \n",
        "# dimentionality of embeddings and with a deeper model.\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear, Tanh\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, letters_len, n_gram_len, emb_dim, h1_size, h2_size):\n",
        "    super().__init__()\n",
        "    self.n_gram_len = n_gram_len\n",
        "    self.emb_dim = emb_dim\n",
        "    self.emb = torch.randn(letters_len, emb_dim)  # dictionary size by embedding dimentionality\n",
        "    self.fc1 = Linear(n_gram_len*emb_dim, h1_size)  # n-gram size times embedding dimentionality by size of hidden 1 \n",
        "    self.fc2 = Linear(h1_size, h2_size)  # size of hidden 1 by size of hidden 2\n",
        "    self.fc3 = Linear(h2_size, letters_len)\n",
        "    self.Tanh = Tanh()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # We need to embed and reshape our input tensor first. \n",
        "    # Quick note on why use .long() here. Theoretically model should work perfectly well with .emb[x].\n",
        "    # The reason for this is that we have only integer values in the dataset.\n",
        "    # However, if we want to use torchsummary to print model details, it will feed the model with float values.\n",
        "    x = self.emb[x.long()].view(-1, self.n_gram_len*self.emb_dim)  \n",
        "    hidden1 = self.Tanh(self.fc1(x))\n",
        "    hidden2 = self.Tanh(self.fc2(hidden1))\n",
        "    logits = self.fc3(hidden2)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "eQksLkwpJ1kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_len = len(letters)  # dictionary size (should be 27)\n",
        "n_gram_len = 3  # what's the size of n-gram (3 or more)\n",
        "emb_dim = 4  # what's the dimentionality of embedding table (in previous ex. it was 2, we want increase that)\n",
        "h1_size = 100  # hidden layer 1 size\n",
        "h2_size = 50  # hidden layer 2 size\n",
        "model = MLP(dict_len, n_gram_len, emb_dim, h1_size, h2_size)"
      ],
      "metadata": {
        "id": "W6klTlPledBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We may print model structure with print method ...\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvEx2KyhW3q-",
        "outputId": "04f85b50-1653-470e-dc10-186b8d7bd878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (fc1): Linear(in_features=12, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=50, bias=True)\n",
            "  (fc3): Linear(in_features=50, out_features=27, bias=True)\n",
            "  (Tanh): Tanh()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... however I prefer using torch-summary where possible: https://github.com/sksq96/pytorch-summary\n",
        "from torchsummary import summary\n",
        "summary(model, (n_gram_len, emb_dim))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neoT0Ps_V2jT",
        "outputId": "969ef4a4-5883-4312-c397-32effab98c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                  [-1, 100]           1,300\n",
            "              Tanh-2                  [-1, 100]               0\n",
            "            Linear-3                   [-1, 50]           5,050\n",
            "              Tanh-4                   [-1, 50]               0\n",
            "            Linear-5                   [-1, 27]           1,377\n",
            "================================================================\n",
            "Total params: 7,727\n",
            "Trainable params: 7,727\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.03\n",
            "Estimated Total Size (MB): 0.03\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "# here we define some hyperparameters.\n",
        "epochs = 600\n",
        "learning_rate = 0.5  # this is initial learning rate - we are going to use scheduler to decrease LR\n",
        "criterion = torch.nn.CrossEntropyLoss()  \n",
        "optimizer = SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# we use scheduler that multiplies LR by gamma every epoch given in milestones\n",
        "scheduler = MultiStepLR(optimizer, milestones=[200,300,400,500], gamma=0.2)\n",
        "\n",
        "# Note that the batch size is already defined in the DataLoaders.\n",
        "print(f'The batch size is {batch_size}')"
      ],
      "metadata": {
        "id": "pv5QR2JtVjrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0199fc20-8f49-4e82-abb0-105ea7850a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The batch size is 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we start the training, let's define a helper function that accepts\n",
        "# data_loader and model and collects (in a form of list) accuracies \n",
        "# for each batch of a data loader. We may then use this function to calculate\n",
        "# accuracy for validation and training datasets to check if there is any overfitting\n",
        "def evaluate(data_loader, model):\n",
        "  accuracy = []\n",
        "  for x, y in data_loader:\n",
        "    preds = model(x)  # logits\n",
        "    predictions = torch.argmax(preds, dim=1).detach().cpu().numpy()  # predictions\n",
        "    batch_accuracy = (predictions==y.detach().cpu().numpy()).sum() / len(y)  # compare predictions with ground truth and calculating accuracy for a batch\n",
        "    accuracy.append(batch_accuracy)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "Ghro9-GAqe8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "tr_losses = []\n",
        "val_accuracies = []\n",
        "tr_accuracies = []\n",
        "\n",
        "start = timer()\n",
        "for epoch in range(1, epochs+1):\n",
        "  for x, y in train_dl:\n",
        "    # forward pass\n",
        "    y_hat = model(x)\n",
        "\n",
        "    # backward pass\n",
        "    loss = criterion(y_hat, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # train accuracy calculated every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "      tr_accuracy = evaluate(train_dl, model) \n",
        "      tr_accuracies.append(np.array(tr_accuracy).sum() / len(tr_accuracy))\n",
        "\n",
        "    # validation accuracy calculated in every epoch\n",
        "    val_accuracy = evaluate(val_dl, model)\n",
        "    val_accuracies.append(np.array(val_accuracy).sum() / len(val_accuracy))\n",
        "  model.train()\n",
        "  \n",
        "  # learning rate decay\n",
        "  scheduler.step()\n",
        "  \n",
        "  # record losses\n",
        "  tr_losses.append(loss.detach().numpy().mean())\n",
        "\n",
        "end = timer()\n",
        "print(f'Learning time {end-start:0.2f}s. Last training loss: {tr_losses[-1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0MDAald9fD6",
        "outputId": "4b8665bd-b9d3-4155-f299-5e958bd973af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning time 2989.88s. Last training loss: 2.1225686073303223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print loss the validation accuracy, then training accuracy (this one collected every 10 epoch only)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(tr_losses)"
      ],
      "metadata": {
        "id": "H0uUymtpec7n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "5638dd93-a817-4b68-fbd9-007d189f6288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff63587f610>]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(val_accuracies)"
      ],
      "metadata": {
        "id": "P6O_7yKiec4E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b4acdb5b-b3ee-4925-a737-7d82bad08ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff63549bd10>]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(tr_accuracies)"
      ],
      "metadata": {
        "id": "bRwUnXvIJ1ha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "b887ead6-5615-419a-a39a-ed35603b5683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff635384bd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We're ready to use our trained model to generate names.\n",
        "for _ in range(15): \n",
        "  prev_gram = torch.tensor([char2idx['.']]*3)  # we start with the initial n-gram of 3 starting characters\n",
        "  pred_word = []  # this will be our predicted word\n",
        "  while True:  # we run this loop until we get the ending '.' character\n",
        "    # we get predicted logits\n",
        "    output = model(prev_gram)  \n",
        "\n",
        "    # we want to turn logits into probability distribution using softmax.\n",
        "    prob_dist = torch.softmax(output, dim=1) \n",
        "\n",
        "    # and then draw from this distribution using multinomial probability distribution\n",
        "    prediction = torch.multinomial(prob_dist, num_samples=1, replacement=True).squeeze(1)  # note: we need to squeeze as multinomial outputs extra dimention\n",
        "    \n",
        "    # calculate predicted character\n",
        "    predicted_ch = idx2char[prediction.item()]  \n",
        "    if predicted_ch == '.':  # if we get the ending character -> we have just predicted a whole word -> break while loop\n",
        "      break\n",
        "    \n",
        "    pred_word.append(predicted_ch)  # add character to the word\n",
        "    \n",
        "    # we need to shift our 3-gram to the left and add newly predicted character to repeat loop\n",
        "    prev_gram = torch.roll(prev_gram, -1)\n",
        "    prev_gram[2] = prediction\n",
        "\n",
        "  print(''.join(pred_word))"
      ],
      "metadata": {
        "id": "j3FXjrABHClN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f57f525-09c6-44e7-a618-d78bdb73997e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mailynoriann\n",
            "eman\n",
            "kumab\n",
            "kammariyah\n",
            "ewslett\n",
            "aranfrean\n",
            "alderret\n",
            "kadalaya\n",
            "joshir\n",
            "yaballoheidy\n",
            "cayziah\n",
            "jon\n",
            "jana\n",
            "mona\n",
            "tya\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49M5SH1biRDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}