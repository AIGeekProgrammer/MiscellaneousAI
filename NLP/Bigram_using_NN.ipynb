{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OAnvyNv_Qhu2taWfGTiZ2ssVaZvXxedo",
      "authorship_tag": "ABX9TyMB2BXsnU5SzIImbzxdETL9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIGeekProgrammer/MiscellaneousAI/blob/main/NLP/Bigram_using_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook: Bigram using NN<br>\n",
        "Author: Szymon Manduk<br>\n",
        "Date: Oct 31, 2022<br>\n",
        "Description: implementing Bigram algorithm using single linear layer - based on the idea presented by A. Karpathy: https://youtu.be/PaCmpygFfXo<br>"
      ],
      "metadata": {
        "id": "oHM7GLj0Ewa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "6_Zbx7NBTiSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79chBQlgHv2O",
        "outputId": "0301d40b-7e56-4281-a9d4-9818e91ad70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a file with names, read into list removing newline \n",
        "# and adding character '.' for the begining and end of a word.\n",
        "words = []\n",
        "with open('/gdrive/My Drive/Test/names.txt', 'r') as f:\n",
        "  for cnt, line in enumerate(f.readlines()):\n",
        "    words.append('.' + line.rstrip('\\n') + '.')\n",
        "words[:5]"
      ],
      "metadata": {
        "id": "W7-kc0QHHv2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c26be9-b662-45c6-d12f-ca25c2e5ac94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.emma.', '.olivia.', '.ava.', '.isabella.', '.sophia.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a list of all unique letters ...\n",
        "s = set()\n",
        "for word in words:\n",
        "  s.update(list(word))\n",
        "letters = sorted(list(s))\n",
        "letters[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ssVFrJGBiOc",
        "outputId": "c2cca00c-df80-4fb5-b4a4-51ab2ba18de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ... to be able to convert from a letter to a number and vice versa.\n",
        "char2idx = {ch:i for i,ch in enumerate(letters)}\n",
        "idx2char = {i:ch for i,ch in enumerate(letters)}\n",
        "print(f'Ex: Index for f is {char2idx[\"f\"]}')\n",
        "print(f'Ex: Character for index 14 is {idx2char[14]}')\n",
        "print(f'Ex: Character for index 0 is {idx2char[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQGPy19FmcC9",
        "outputId": "1c427655-f3c2-487d-91a9-c81b1e272fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ex: Index for f is 6\n",
            "Ex: Character for index 14 is n\n",
            "Ex: Character for index 0 is .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we're ready to build data and label tensors let's start with the first word only.\n",
        "x, y = [], []\n",
        "for word in words[:1]:  \n",
        "  for ch1, ch2 in zip(word, word[1:]):\n",
        "    x.append(char2idx[ch1])\n",
        "    y.append(char2idx[ch2])\n",
        "X = torch.tensor(x, dtype=torch.int64) # int64 necessary to later use F.one_hot function\n",
        "Y = torch.tensor(y, dtype=torch.int64) # int64 necessary to later use F.one_hot function\n",
        "print(f'Length of the dataset: {len(X)}')\n",
        "print(f'Shape of the dataset: {X.shape}')\n",
        "print(f'Data X: {X}')\n",
        "print(f'Labels Y: {Y}')\n",
        "\n",
        "# We cannot feed a neural network with numerical values. We need to turn them into one-hot encoded version.\n",
        "from torch.nn.functional import one_hot\n",
        "X = one_hot(X, 27).float()\n",
        "print(X.shape)\n",
        "print(X)\n",
        "print(X.dtype)\n",
        "print(Y.shape)\n",
        "print(Y)\n",
        "print(Y.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlkRs4Ijwepf",
        "outputId": "fecc7d15-098e-49c4-d459-1b7815be65c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the dataset: 5\n",
            "Shape of the dataset: torch.Size([5])\n",
            "Data X: tensor([ 0,  5, 13, 13,  1])\n",
            "Labels Y: tensor([ 5, 13, 13,  1,  0])\n",
            "torch.Size([5, 27])\n",
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "torch.float32\n",
            "torch.Size([5])\n",
            "tensor([ 5, 13, 13,  1,  0])\n",
            "torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looks ok, execute on the full dataset.\n",
        "x, y = [], []\n",
        "for word in words:  \n",
        "  for ch1, ch2 in zip(word, word[1:]):\n",
        "    x.append(char2idx[ch1])\n",
        "    y.append(char2idx[ch2])\n",
        "X = torch.tensor(x, dtype=torch.int64) \n",
        "Y = torch.tensor(y, dtype=torch.int64) \n",
        "print(f'Length of the dataset: {len(X)}')\n",
        "print(f'Shape of the dataset: {X.shape}')\n",
        "print(f'Data X: {X}')\n",
        "print(f'Labels Y: {Y}')\n",
        "\n",
        "X = one_hot(X, 27).float()\n",
        "print(X.shape, X.dtype)\n",
        "print(Y.shape, Y.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h05emRSbOSIv",
        "outputId": "1232ccf6-ec4a-4a71-c090-86e886acb905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of the dataset: 228146\n",
            "Shape of the dataset: torch.Size([228146])\n",
            "Data X: tensor([ 0,  5, 13,  ..., 25, 26, 24])\n",
            "Labels Y: tensor([ 5, 13, 13,  ..., 26, 24,  0])\n",
            "torch.Size([228146, 27]) torch.float32\n",
            "torch.Size([228146]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize matrix W.\n",
        "W = torch.randn((X.shape[1], X.shape[1]), requires_grad=True)\n",
        "print(W.shape)\n",
        "print(W[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkbQoFNwO8fg",
        "outputId": "e815cedd-c67f-4fd4-d6cd-00aea2577b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([27, 27])\n",
            "tensor([[-0.4908,  1.0303, -0.2841, -0.8224, -0.3459, -1.2313, -1.2028,  0.2898,\n",
            "          0.3738,  0.5329, -0.6490, -0.0796,  0.3531, -0.8806,  0.2581,  0.4152,\n",
            "         -0.0859, -0.7320, -0.0279, -1.3311, -1.8962, -0.3533, -1.2447,  1.8507,\n",
            "          1.2245,  0.3065, -1.2184],\n",
            "        [ 1.1710, -0.2254, -1.7892,  0.2823,  0.6517, -0.3705, -0.5825, -0.1892,\n",
            "          1.0506, -2.7771, -0.4508, -1.1691, -1.2294,  1.5214,  0.9521, -0.9905,\n",
            "         -1.2825, -0.9770,  0.2860,  0.6390, -0.7895,  0.5103,  1.2528, -1.5229,\n",
            "         -0.1985,  0.7597,  0.6804]], grad_fn=<SliceBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's look at unnormalized probabilities.\n",
        "y_hat = X @ W\n",
        "print(y_hat.shape)\n",
        "print(y_hat[0:3])\n",
        "print(y_hat[1].sum()) # they won't sum to zero, apart some rare cases"
      ],
      "metadata": {
        "id": "9JNKQ459wead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d7329f-345a-4acc-c618-57a60e3aa064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([228146, 27])\n",
            "tensor([[-0.4908,  1.0303, -0.2841, -0.8224, -0.3459, -1.2313, -1.2028,  0.2898,\n",
            "          0.3738,  0.5329, -0.6490, -0.0796,  0.3531, -0.8806,  0.2581,  0.4152,\n",
            "         -0.0859, -0.7320, -0.0279, -1.3311, -1.8962, -0.3533, -1.2447,  1.8507,\n",
            "          1.2245,  0.3065, -1.2184],\n",
            "        [ 0.5376, -1.1097,  0.3294, -0.4814,  0.7639,  1.2436,  0.1327,  0.4370,\n",
            "          0.3091,  0.9484, -0.6145,  0.1337, -1.6202, -0.8083, -0.1803, -0.6315,\n",
            "         -3.6680,  0.7417, -0.5151,  0.2471,  0.3862, -0.1556,  1.2671,  0.5975,\n",
            "         -0.2727, -0.1975,  0.1396],\n",
            "        [ 1.4480,  0.4760,  0.7686, -1.6357,  1.1872, -0.3883, -0.2518,  1.8967,\n",
            "          1.1371, -0.7463, -0.3349,  1.4677, -0.2780,  0.8616,  1.0619, -1.8788,\n",
            "         -1.8582, -0.5489, -0.2618,  0.0728,  0.6515, -1.4452,  0.2392, -1.7503,\n",
            "          1.8052,  0.9609, -1.3850]], grad_fn=<SliceBackward0>)\n",
            "tensor(-2.0399, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Then let's try with sotfmax'ed (normalized) probabilites.\n",
        "from torch.nn.functional import softmax\n",
        "y_hat_norm = softmax(X @ W, dim=1)\n",
        "print(y_hat_norm.shape)\n",
        "print(y_hat_norm[0:3])\n",
        "print(y_hat_norm[1].sum()) # they should sum to zero as data are softmaxed"
      ],
      "metadata": {
        "id": "s_lN_PXhSYpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba862a3a-8c7a-4d74-ebf1-5bcafaf084e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([228146, 27])\n",
            "tensor([[0.0193, 0.0884, 0.0238, 0.0139, 0.0223, 0.0092, 0.0095, 0.0422, 0.0459,\n",
            "         0.0538, 0.0165, 0.0291, 0.0449, 0.0131, 0.0409, 0.0478, 0.0290, 0.0152,\n",
            "         0.0307, 0.0083, 0.0047, 0.0222, 0.0091, 0.2008, 0.1074, 0.0429, 0.0093],\n",
            "        [0.0497, 0.0096, 0.0404, 0.0180, 0.0624, 0.1008, 0.0332, 0.0450, 0.0396,\n",
            "         0.0750, 0.0157, 0.0332, 0.0057, 0.0129, 0.0243, 0.0154, 0.0007, 0.0610,\n",
            "         0.0174, 0.0372, 0.0427, 0.0249, 0.1031, 0.0528, 0.0221, 0.0238, 0.0334],\n",
            "        [0.0859, 0.0325, 0.0435, 0.0039, 0.0662, 0.0137, 0.0157, 0.1345, 0.0629,\n",
            "         0.0096, 0.0144, 0.0876, 0.0153, 0.0478, 0.0584, 0.0031, 0.0031, 0.0117,\n",
            "         0.0155, 0.0217, 0.0387, 0.0048, 0.0256, 0.0035, 0.1227, 0.0528, 0.0051]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "tensor(1.0000, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass.\n",
        "logits = X @ W\n",
        "counts = logits.exp()\n",
        "probs = counts / counts.sum(1, keepdim=True)\n",
        "loss = -probs[torch.arange(len(X)), Y].log().mean()\n",
        "print(f'Loss: {loss.item()}')\n",
        "\n",
        "# Backward pass, previously zeroing gradients.\n",
        "W.grad = None\n",
        "loss.backward()\n",
        "W.grad[0:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QqEpNQ8KHYH",
        "outputId": "c923ef32-512b-4f57-d109-88ff2fb591a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.607822895050049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0027, -0.0069, -0.0024, -0.0048, -0.0043, -0.0054, -0.0005,  0.0030,\n",
              "          0.0026,  0.0050, -0.0083, -0.0089, -0.0006, -0.0093,  0.0007,  0.0050,\n",
              "          0.0018,  0.0017, -0.0029, -0.0078, -0.0051,  0.0028, -0.0004,  0.0269,\n",
              "          0.0145,  0.0037, -0.0028]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Forward pass (using CrossEntropyLoss).\n",
        "  from torch.nn import CrossEntropyLoss\n",
        "  criterion = CrossEntropyLoss()  # we use this loss on unnormalized probabilities \n",
        "\n",
        "  y_hat = X @ W\n",
        "\n",
        "  # Calculate and print loss.\n",
        "  loss = criterion(y_hat, Y)\n",
        "  print(f'Loss: {loss.item()}')\n",
        "\n",
        "  # Backward pass, previously zeroing gradients.\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  W.grad[0:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcp8nJ5CbSFr",
        "outputId": "6f1d9465-8744-4f08-d5ca-49a3e181fa32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 3.6078226566314697\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0027, -0.0069, -0.0024, -0.0048, -0.0043, -0.0054, -0.0005,  0.0030,\n",
              "          0.0026,  0.0050, -0.0083, -0.0089, -0.0006, -0.0093,  0.0007,  0.0050,\n",
              "          0.0018,  0.0017, -0.0029, -0.0078, -0.0051,  0.0028, -0.0004,  0.0269,\n",
              "          0.0145,  0.0037, -0.0028]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # Forward pass (let's use NLLLoss).\n",
        "  from torch.nn import NLLLoss\n",
        "  criterion = NLLLoss() # with NLLLoss we need softmax'ed values \n",
        "  y_hat = softmax(X @ W, dim=1)\n",
        "\n",
        "  # Calculate and print loss.\n",
        "  loss = criterion(y_hat, Y)\n",
        "  print(f'Loss: {loss.item()}')\n",
        "\n",
        "  # Backward pass, previously zeroing gradients.\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  W.grad[0:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHi1IU-fb29j",
        "outputId": "79f55ae1-c8be-4289-b6fe-034bebfa1083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: -0.040998440235853195\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 9.2852e-05, -1.2840e-03, -2.1797e-05, -2.7065e-05, -5.8073e-05,\n",
              "         -1.7537e-05,  2.8234e-05,  7.9035e-05,  4.4753e-05,  1.1917e-04,\n",
              "         -9.5801e-05, -2.3839e-04, -9.3591e-05, -8.2644e-05, -8.8378e-06,\n",
              "          1.4721e-04,  7.3826e-05,  6.6834e-05, -7.2955e-05, -3.5020e-05,\n",
              "         -4.3893e-06,  9.8962e-05,  2.8710e-05,  6.9510e-04,  4.5305e-04,\n",
              "          1.0554e-04,  6.8570e-06]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This allows us to optimize this simple model by iterating few times,\n",
        "# calculating forward pass by matrix multiplication (and in case we want to use \n",
        "# NLLLoss aplying softmax), calculate loss, do the backward pass\n",
        "# AND finally: update parameters using calculated gradient.\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "for i in range(200):\n",
        "  # forward pass\n",
        "  y_hat = X @ W\n",
        "\n",
        "  # calculate and print loss\n",
        "  loss = criterion(y_hat, Y)\n",
        "  if (i+1) % 10 == 0:\n",
        "    print(f'Loss at {i+1} iteration: {loss.item()}')\n",
        "\n",
        "  # backward pass, previously zeroing gradients\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # parameters update\n",
        "  W.data += -20 * W.grad\n"
      ],
      "metadata": {
        "id": "daWqJjPgZi9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bf7f30-08ad-4eb3-f72d-64a46f2b9035"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at 10 iteration: 2.94762921333313\n",
            "Loss at 20 iteration: 2.727717161178589\n",
            "Loss at 30 iteration: 2.642289638519287\n",
            "Loss at 40 iteration: 2.5973644256591797\n",
            "Loss at 50 iteration: 2.5690245628356934\n",
            "Loss at 60 iteration: 2.549506902694702\n",
            "Loss at 70 iteration: 2.5353474617004395\n",
            "Loss at 80 iteration: 2.5246477127075195\n",
            "Loss at 90 iteration: 2.516289710998535\n",
            "Loss at 100 iteration: 2.5095908641815186\n",
            "Loss at 110 iteration: 2.5041098594665527\n",
            "Loss at 120 iteration: 2.4995474815368652\n",
            "Loss at 130 iteration: 2.4956905841827393\n",
            "Loss at 140 iteration: 2.4923861026763916\n",
            "Loss at 150 iteration: 2.4895222187042236\n",
            "Loss at 160 iteration: 2.487015724182129\n",
            "Loss at 170 iteration: 2.484804153442383\n",
            "Loss at 180 iteration: 2.4828379154205322\n",
            "Loss at 190 iteration: 2.4810805320739746\n",
            "Loss at 200 iteration: 2.4795010089874268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we are ready to use our model to predict few words\n",
        "for _ in range(15):\n",
        "  pred = torch.tensor([char2idx['.']])  # we start with the initial character\n",
        "  pred_word = []  # but we do not add the initial character to the result\n",
        "  while True:\n",
        "    prev_char = one_hot(pred, num_classes=27).float()  # one-hot previous character\n",
        "    p = softmax(prev_char @ W, dim=1)  # predict probability\n",
        "    pred = torch.multinomial(p, num_samples=1, replacement=True).squeeze(1)  # sample from multinomial, note: we need to squeeze as multinomial adds extra dimention\n",
        "    pred_ch = idx2char[pred.item()]  # calculate character\n",
        "    if pred_ch == '.':  # if ending character -> break while True\n",
        "      break\n",
        "    pred_word.append(pred_ch)  # add character to the list\n",
        "  print(''.join(pred_word))"
      ],
      "metadata": {
        "id": "j3FXjrABHClN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6a1609-b00d-4121-b6d4-925a796582be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myleirapeiosaenie\n",
            "loa\n",
            "enige\n",
            "gavtavise\n",
            "jan\n",
            "aho\n",
            "luramrison\n",
            "atlea\n",
            "jena\n",
            "ton\n",
            "aanieanaa\n",
            "an\n",
            "le\n",
            "syada\n",
            "n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "49M5SH1biRDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}