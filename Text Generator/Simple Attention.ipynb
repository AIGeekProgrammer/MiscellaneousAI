{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook with a very simple attention mechanism\n",
    "# Author: Szymon Manduk\n",
    "# Date: Feb 09, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have input vector X of 10 tokens (rows), each token embedded in 32 dimensions (columns)\n",
    "X = torch.randn(10, 32)\n",
    "\n",
    "# Query parameters matrix Q is 32x32\n",
    "pQ = torch.randn(32, 32)\n",
    "\n",
    "# Key parameters matrix K is 32x32\n",
    "pK = torch.randn(32, 32)\n",
    "\n",
    "# Value parameters matrix V is 32x32\n",
    "pV = torch.randn(32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 of attention: multiply parametric query matrix pQ with X\n",
    "# pQ is 32x32, X is 10x32, so the result is 10x32\n",
    "Q = torch.matmul(X, pQ)\n",
    "\n",
    "# step 2 of attention: multiply parametric key matrix pK with X\n",
    "# pK is 32x32, X is 10x32, so the result is 10x32\n",
    "K = torch.matmul(X, pK)\n",
    "\n",
    "# step 3 of attention: multiply parametric value matrix pV with X\n",
    "# pV is 32x32, X is 10x32, so the result is 10x32\n",
    "V = torch.matmul(X, pV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4 of attention: calculate the attention weights\n",
    "# Q is 10x32, K is 10x32, so the result is 10x10\n",
    "# we need to transpose K to make it 32x10\n",
    "# we need to divide by sqrt(32) to normalize the weights\n",
    "weights = torch.matmul(Q, torch.transpose(K, 0, 1)) / torch.sqrt(torch.tensor(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5 of attention: apply softmax to the weights\n",
    "# weights is 10x10, so the result is 10x10\n",
    "weights = torch.softmax(weights, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6 of attention: multiply the weights with the values\n",
    "# weights is 10x10, V is 10x32, so the result is 10x32\n",
    "output = torch.matmul(weights, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all those operation can be done in one line\n",
    "output2 = torch.matmul(torch.softmax(torch.matmul(torch.matmul(X, pQ), torch.transpose(torch.matmul(X, pK), 0, 1)) / torch.sqrt(torch.tensor(32)), dim=1), torch.matmul(X, pV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(output, output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7028, -0.6439,  1.5270,  0.5268],\n",
       "        [-0.9692, -0.2577, -0.2966,  0.1289],\n",
       "        [-0.8208, -1.0258,  0.3171,  0.9205],\n",
       "        [-0.6280,  0.3318, -0.2130,  0.0966]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(4,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stack tensors one next to another along x axis, keeping dimension 0\n",
    "\n",
    "torch.stack([t, t, t, t, t], dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.empty(t.shape[0], 0)\n",
    "for _ in range(5):\n",
    "    out = torch.cat((out, t), dim=1)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7028, -0.6439,  1.5270,  0.5268, -0.7028, -0.6439,  1.5270,  0.5268,\n",
       "         -0.7028, -0.6439,  1.5270,  0.5268, -0.7028, -0.6439,  1.5270,  0.5268,\n",
       "         -0.7028, -0.6439,  1.5270,  0.5268],\n",
       "        [-0.9692, -0.2577, -0.2966,  0.1289, -0.9692, -0.2577, -0.2966,  0.1289,\n",
       "         -0.9692, -0.2577, -0.2966,  0.1289, -0.9692, -0.2577, -0.2966,  0.1289,\n",
       "         -0.9692, -0.2577, -0.2966,  0.1289],\n",
       "        [-0.8208, -1.0258,  0.3171,  0.9205, -0.8208, -1.0258,  0.3171,  0.9205,\n",
       "         -0.8208, -1.0258,  0.3171,  0.9205, -0.8208, -1.0258,  0.3171,  0.9205,\n",
       "         -0.8208, -1.0258,  0.3171,  0.9205],\n",
       "        [-0.6280,  0.3318, -0.2130,  0.0966, -0.6280,  0.3318, -0.2130,  0.0966,\n",
       "         -0.6280,  0.3318, -0.2130,  0.0966, -0.6280,  0.3318, -0.2130,  0.0966,\n",
       "         -0.6280,  0.3318, -0.2130,  0.0966]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8136, -0.0446,  0.4911,  0.8046,  1.3285,  0.8012, -0.8975, -0.2174],\n",
       "        [ 0.3294, -1.0827, -0.6132, -1.1408, -0.8309,  0.4316,  1.2974, -2.2530],\n",
       "        [-0.0656, -0.1330,  0.3573,  0.7055, -0.5388, -0.6816, -0.1902,  0.2447],\n",
       "        [ 0.4911, -0.2504,  0.3871,  0.4993, -0.2478, -1.0736,  1.4788,  0.0781],\n",
       "        [ 0.7119, -0.7082, -1.5157,  0.0637, -1.2900,  0.1505, -2.1048, -1.0875],\n",
       "        [ 0.4207,  0.6608, -0.9840, -1.2339,  0.1558,  0.1938, -1.0271, -1.6201],\n",
       "        [-0.2279, -1.0045, -0.4449,  0.7974,  1.1239, -1.1883, -0.6843,  1.9002],\n",
       "        [-0.4206,  0.3109,  1.0727,  0.6116, -3.4153,  0.3660, -1.5124, -1.8122]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.randn(8,8)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones(8,8), diagonal=0)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8136,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.3294, -1.0827,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0656, -0.1330,  0.3573,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.4911, -0.2504,  0.3871,  0.4993,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.7119, -0.7082, -1.5157,  0.0637, -1.2900,    -inf,    -inf,    -inf],\n",
       "        [ 0.4207,  0.6608, -0.9840, -1.2339,  0.1558,  0.1938,    -inf,    -inf],\n",
       "        [-0.2279, -1.0045, -0.4449,  0.7974,  1.1239, -1.1883, -0.6843,    -inf],\n",
       "        [-0.4206,  0.3109,  1.0727,  0.6116, -3.4153,  0.3660, -1.5124, -1.8122]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = m.masked_fill(mask == 0, float('-inf'))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8041, 0.1959, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2889, 0.2701, 0.4410, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2953, 0.1407, 0.2662, 0.2978, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4981, 0.1204, 0.0537, 0.2605, 0.0673, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2341, 0.2976, 0.0575, 0.0447, 0.1796, 0.1865, 0.0000, 0.0000],\n",
       "        [0.1007, 0.0463, 0.0810, 0.2807, 0.3890, 0.0385, 0.0638, 0.0000],\n",
       "        [0.0759, 0.1578, 0.3381, 0.2132, 0.0038, 0.1668, 0.0255, 0.0189]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(m, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da32ec28290c759936d4243a9dab89f7c584810c5583b2f09fcc9401c111f823"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
